{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re    # for regular expressions \n",
    "import nltk  # for text manipulation \n",
    "import string \n",
    "import warnings \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_colwidth\", 200) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train  = pd.read_csv('./data/train.csv',encoding='utf-8') \n",
    "test = pd.read_csv('./data/test.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7274, 3), (1819, 6))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi = train.append(test, ignore_index=True) \n",
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "def fix_Plan(tweet):\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\n",
    "                          \" \",          # Replace all non-letters with spaces\n",
    "                          str(tweet))     # Column and row to search \n",
    "    letters_only = re.sub(r'@\\w+', '', letters_only)\n",
    "    letters_only = re.sub('#', '', letters_only)\n",
    "    letters_only = re.sub('RT[\\s]+', '', letters_only)\n",
    "    letters_only = re.sub(' +', ' ',letters_only )\n",
    "    letters_only = re.sub(r\"\\'s\", \" \\'s\", letters_only)\n",
    "    letters_only = re.sub(r\"\\'ve\", \" \\'ve\", letters_only)\n",
    "    letters_only = re.sub(r\"n\\'t\", \" n\\'t\", letters_only)\n",
    "    letters_only = re.sub(r\"\\'re\", \" \\'re\", letters_only)\n",
    "    letters_only = re.sub(r\"\\'d\", \" \\'d\", letters_only)\n",
    "    letters_only = re.sub(r\"\\'ll\", \" \\'ll\", letters_only)\n",
    "    letters_only = re.sub(r\",\", \" , \", letters_only)\n",
    "    letters_only = re.sub(r\"!\", \" ! \", letters_only)\n",
    "    letters_only = re.sub(r\"\\(\", \" \\( \", letters_only)\n",
    "    letters_only = re.sub(r\"\\)\", \" \\) \", letters_only)\n",
    "    letters_only = re.sub(r\"\\?\", \" \\? \", letters_only)\n",
    "    letters_only = re.sub(r\"\\s{2,}\", \" \", letters_only)\n",
    "    words = letters_only.lower().split()     \n",
    "    stops = list(set(stopwords.words(\"english\")))+['link','quot','amp','gt','mention'] +list(punctuation)+['``', \"'s\", \"...\", \"n't\",\"n't\", \"not\", \"no\"]\n",
    "    \n",
    "    meaningful_words = [w for w in words if not w in stops]      \n",
    "    return (\" \".join(meaningful_words))    \n",
    "         \n",
    "col_Plan = fix_Plan(combi[\"tweet\"][0])    \n",
    "num_responses = combi[\"tweet\"].size    \n",
    "clean_Plan_responses = []\n",
    "\n",
    "for i in range(0,num_responses):\n",
    "    clean_Plan_responses.append(fix_Plan(combi[\"tweet\"][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combi['tidy_tweet']=pd.DataFrame(clean_Plan_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sxswnui sxsw apple defining language touch different dialects becoming smaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should be light, funny &amp;amp; innovative, with exceptions for significant occasions. #GoogleDoodle #sxsw</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>learning ab google doodles doodles light funny innovative exceptions significant occasions googledoodle sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing the show in yrs RT @mention &amp;quot;At #SXSW, Apple schools the mkt experts&amp;quot;  {link}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one face ex stealing show yrs sxsw apple schools mkt experts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome if it didn't crash every 10mins during extended browsing. #Fuckit #Illmakeitwork</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iphone sxsw app would b pretty awesome crash every mins extended browsing fuckit illmakeitwork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting for the new iPad #SXSW  {link}</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>line outside apple store austin waiting new ipad sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>1550</td>\n",
       "      <td>@mention @mention @mention Hmmm....how fast can #apple build a new store in time for #sxsw  {link}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hmmm fast apple build new store time sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>1933</td>\n",
       "      <td>Samsung Galaxy S II Appears At FCC And Team Android #SXSW Party {link} via @mention</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>samsung galaxy ii appears fcc team android sxsw party via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>9052</td>\n",
       "      <td>@mention You could buy a new iPad 2 tmrw at the Apple pop-up store at #sxsw: {link}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>could buy new ipad tmrw apple pop store sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>4219</td>\n",
       "      <td>Wow very long queue of people at apple pop up store now, some have bought 3 iPads! #sxsw@mention Room#NokiaConnects</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wow long queue people apple pop store bought ipads sxsw room nokiaconnects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>7210</td>\n",
       "      <td>Privacy Could Headline Google Circles Social Network Reveal Later Today [Social Networks] {link} #ACLU #GoogleCircles #SXSW</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>privacy could headline google circles social network reveal later today social networks aclu googlecircles sxsw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id  \\\n",
       "0         1701   \n",
       "1         1851   \n",
       "2         2689   \n",
       "3         4525   \n",
       "4         3604   \n",
       "...        ...   \n",
       "9088      1550   \n",
       "9089      1933   \n",
       "9090      9052   \n",
       "9091      4219   \n",
       "9092      7210   \n",
       "\n",
       "                                                                                                                                                tweet  \\\n",
       "0                                                           #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller   \n",
       "1     Learning ab Google doodles! All doodles should be light, funny &amp; innovative, with exceptions for significant occasions. #GoogleDoodle #sxsw   \n",
       "2                one of the most in-your-face ex. of stealing the show in yrs RT @mention &quot;At #SXSW, Apple schools the mkt experts&quot;  {link}   \n",
       "3                       This iPhone #SXSW app would b pretty awesome if it didn't crash every 10mins during extended browsing. #Fuckit #Illmakeitwork   \n",
       "4                                                                       Line outside the Apple store in Austin waiting for the new iPad #SXSW  {link}   \n",
       "...                                                                                                                                               ...   \n",
       "9088                                               @mention @mention @mention Hmmm....how fast can #apple build a new store in time for #sxsw  {link}   \n",
       "9089                                                              Samsung Galaxy S II Appears At FCC And Team Android #SXSW Party {link} via @mention   \n",
       "9090                                                              @mention You could buy a new iPad 2 tmrw at the Apple pop-up store at #sxsw: {link}   \n",
       "9091                              Wow very long queue of people at apple pop up store now, some have bought 3 iPads! #sxsw@mention Room#NokiaConnects   \n",
       "9092                      Privacy Could Headline Google Circles Social Network Reveal Later Today [Social Networks] {link} #ACLU #GoogleCircles #SXSW   \n",
       "\n",
       "      sentiment  Unnamed: 3 Unnamed: 4 Unnamed: 5  \\\n",
       "0             1         NaN        NaN        NaN   \n",
       "1             1         NaN        NaN        NaN   \n",
       "2             2         NaN        NaN        NaN   \n",
       "3             0         NaN        NaN        NaN   \n",
       "4             1         NaN        NaN        NaN   \n",
       "...         ...         ...        ...        ...   \n",
       "9088          2         NaN        NaN        NaN   \n",
       "9089          1         NaN        NaN        NaN   \n",
       "9090          2         NaN        NaN        NaN   \n",
       "9091          1         NaN        NaN        NaN   \n",
       "9092          1         NaN        NaN        NaN   \n",
       "\n",
       "                                                                                                           tidy_tweet  \n",
       "0                                      sxswnui sxsw apple defining language touch different dialects becoming smaller  \n",
       "1        learning ab google doodles doodles light funny innovative exceptions significant occasions googledoodle sxsw  \n",
       "2                                                        one face ex stealing show yrs sxsw apple schools mkt experts  \n",
       "3                      iphone sxsw app would b pretty awesome crash every mins extended browsing fuckit illmakeitwork  \n",
       "4                                                               line outside apple store austin waiting new ipad sxsw  \n",
       "...                                                                                                               ...  \n",
       "9088                                                                        hmmm fast apple build new store time sxsw  \n",
       "9089                                                        samsung galaxy ii appears fcc team android sxsw party via  \n",
       "9090                                                                     could buy new ipad tmrw apple pop store sxsw  \n",
       "9091                                       wow long queue people apple pop store bought ipads sxsw room nokiaconnects  \n",
       "9092  privacy could headline google circles social network reveal later today social networks aclu googlecircles sxsw  \n",
       "\n",
       "[9093 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                     [sxswnui, sxsw, apple, defining, language, touch, different, dialects, becoming, smaller]\n",
       "1    [learning, ab, google, doodles, doodles, light, funny, innovative, exceptions, significant, occasions, googledoodle, sxsw]\n",
       "2                                                      [one, face, ex, stealing, show, yrs, sxsw, apple, schools, mkt, experts]\n",
       "3                 [iphone, sxsw, app, would, b, pretty, awesome, crash, every, mins, extended, browsing, fuckit, illmakeitwork]\n",
       "4                                                               [line, outside, apple, store, austin, waiting, new, ipad, sxsw]\n",
       "Name: tidy_tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import * \n",
    "stemmer = PorterStemmer() \n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])    \n",
    "combi['tidy_tweet'] = tokenized_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>tidy_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1701</td>\n",
       "      <td>#sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sxswnui sxsw appl defin languag touch differ dialect becom smaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1851</td>\n",
       "      <td>Learning ab Google doodles! All doodles should be light, funny &amp;amp; innovative, with exceptions for significant occasions. #GoogleDoodle #sxsw</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>learn ab googl doodl doodl light funni innov except signific occas googledoodl sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2689</td>\n",
       "      <td>one of the most in-your-face ex. of stealing the show in yrs RT @mention &amp;quot;At #SXSW, Apple schools the mkt experts&amp;quot;  {link}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one face ex steal show yr sxsw appl school mkt expert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4525</td>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome if it didn't crash every 10mins during extended browsing. #Fuckit #Illmakeitwork</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>iphon sxsw app would b pretti awesom crash everi min extend brows fuckit illmakeitwork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3604</td>\n",
       "      <td>Line outside the Apple store in Austin waiting for the new iPad #SXSW  {link}</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>line outsid appl store austin wait new ipad sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9088</th>\n",
       "      <td>1550</td>\n",
       "      <td>@mention @mention @mention Hmmm....how fast can #apple build a new store in time for #sxsw  {link}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hmmm fast appl build new store time sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9089</th>\n",
       "      <td>1933</td>\n",
       "      <td>Samsung Galaxy S II Appears At FCC And Team Android #SXSW Party {link} via @mention</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>samsung galaxi ii appear fcc team android sxsw parti via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9090</th>\n",
       "      <td>9052</td>\n",
       "      <td>@mention You could buy a new iPad 2 tmrw at the Apple pop-up store at #sxsw: {link}</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>could buy new ipad tmrw appl pop store sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9091</th>\n",
       "      <td>4219</td>\n",
       "      <td>Wow very long queue of people at apple pop up store now, some have bought 3 iPads! #sxsw@mention Room#NokiaConnects</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wow long queue peopl appl pop store bought ipad sxsw room nokiaconnect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9092</th>\n",
       "      <td>7210</td>\n",
       "      <td>Privacy Could Headline Google Circles Social Network Reveal Later Today [Social Networks] {link} #ACLU #GoogleCircles #SXSW</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>privaci could headlin googl circl social network reveal later today social network aclu googlecircl sxsw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9093 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id  \\\n",
       "0         1701   \n",
       "1         1851   \n",
       "2         2689   \n",
       "3         4525   \n",
       "4         3604   \n",
       "...        ...   \n",
       "9088      1550   \n",
       "9089      1933   \n",
       "9090      9052   \n",
       "9091      4219   \n",
       "9092      7210   \n",
       "\n",
       "                                                                                                                                                tweet  \\\n",
       "0                                                           #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller   \n",
       "1     Learning ab Google doodles! All doodles should be light, funny &amp; innovative, with exceptions for significant occasions. #GoogleDoodle #sxsw   \n",
       "2                one of the most in-your-face ex. of stealing the show in yrs RT @mention &quot;At #SXSW, Apple schools the mkt experts&quot;  {link}   \n",
       "3                       This iPhone #SXSW app would b pretty awesome if it didn't crash every 10mins during extended browsing. #Fuckit #Illmakeitwork   \n",
       "4                                                                       Line outside the Apple store in Austin waiting for the new iPad #SXSW  {link}   \n",
       "...                                                                                                                                               ...   \n",
       "9088                                               @mention @mention @mention Hmmm....how fast can #apple build a new store in time for #sxsw  {link}   \n",
       "9089                                                              Samsung Galaxy S II Appears At FCC And Team Android #SXSW Party {link} via @mention   \n",
       "9090                                                              @mention You could buy a new iPad 2 tmrw at the Apple pop-up store at #sxsw: {link}   \n",
       "9091                              Wow very long queue of people at apple pop up store now, some have bought 3 iPads! #sxsw@mention Room#NokiaConnects   \n",
       "9092                      Privacy Could Headline Google Circles Social Network Reveal Later Today [Social Networks] {link} #ACLU #GoogleCircles #SXSW   \n",
       "\n",
       "      sentiment  Unnamed: 3 Unnamed: 4 Unnamed: 5  \\\n",
       "0             1         NaN        NaN        NaN   \n",
       "1             1         NaN        NaN        NaN   \n",
       "2             2         NaN        NaN        NaN   \n",
       "3             0         NaN        NaN        NaN   \n",
       "4             1         NaN        NaN        NaN   \n",
       "...         ...         ...        ...        ...   \n",
       "9088          2         NaN        NaN        NaN   \n",
       "9089          1         NaN        NaN        NaN   \n",
       "9090          2         NaN        NaN        NaN   \n",
       "9091          1         NaN        NaN        NaN   \n",
       "9092          1         NaN        NaN        NaN   \n",
       "\n",
       "                                                                                                    tidy_tweet  \n",
       "0                                           sxswnui sxsw appl defin languag touch differ dialect becom smaller  \n",
       "1                          learn ab googl doodl doodl light funni innov except signific occas googledoodl sxsw  \n",
       "2                                                        one face ex steal show yr sxsw appl school mkt expert  \n",
       "3                       iphon sxsw app would b pretti awesom crash everi min extend brows fuckit illmakeitwork  \n",
       "4                                                             line outsid appl store austin wait new ipad sxsw  \n",
       "...                                                                                                        ...  \n",
       "9088                                                                  hmmm fast appl build new store time sxsw  \n",
       "9089                                                  samsung galaxi ii appear fcc team android sxsw parti via  \n",
       "9090                                                               could buy new ipad tmrw appl pop store sxsw  \n",
       "9091                                    wow long queue peopl appl pop store bought ipad sxsw room nokiaconnect  \n",
       "9092  privaci could headlin googl circl social network reveal later today social network aclu googlecircl sxsw  \n",
       "\n",
       "[9093 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 1600)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1600, stop_words='english') \n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_tweet']) \n",
    "bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 1600)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1600, stop_words='english') \n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing \n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            size=500, # desired no. of features/independent variables\n",
    "            window=10, # context window size\n",
    "            min_count=2,\n",
    "            sg = 2, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 2, # no.of cores\n",
    "            seed = 34) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1357406, 1880020)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gswsxsw', 0.45801064372062683),\n",
       " ('fret', 0.4545798897743225),\n",
       " ('cutsi', 0.44650691747665405),\n",
       " ('thingsthatdontgotogeth', 0.443882554769516),\n",
       " ('appropri', 0.4373396337032318),\n",
       " ('sadli', 0.4284476041793823),\n",
       " ('ooooo', 0.42779985070228577),\n",
       " ('caught', 0.42740410566329956),\n",
       " ('stack', 0.42599618434906006),\n",
       " ('plung', 0.4224025309085846)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"ipad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kek', 0.5281490087509155),\n",
       " ('howard', 0.526698112487793),\n",
       " ('incorrect', 0.52326500415802),\n",
       " ('juwan', 0.5190228223800659),\n",
       " ('increas', 0.5190190672874451),\n",
       " ('reson', 0.4991717040538788),\n",
       " ('socialflow', 0.4929943084716797),\n",
       " ('marisa', 0.48851653933525085),\n",
       " ('nope', 0.484977126121521),\n",
       " ('splendor', 0.48417308926582336)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_w2v.wv.most_similar(positive=\"googl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary  continue\n",
    "            if count != 0:\n",
    "                vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 500)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 500)\n",
    "    wordvec_df = pd.DataFrame(wordvec_arrays) \n",
    "    wordvec_df.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tqdm\\std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, [\"tweet_\" + str(i)]))\n",
    "    return output\n",
    "labeled_tweets = add_label(tokenized_tweet) # label all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['sxswnui', 'sxsw', 'appl', 'defin', 'languag', 'touch', 'differ', 'dialect', 'becom', 'smaller'], tags=['tweet_0']),\n",
       " LabeledSentence(words=['learn', 'ab', 'googl', 'doodl', 'doodl', 'light', 'funni', 'innov', 'except', 'signific', 'occas', 'googledoodl', 'sxsw'], tags=['tweet_1']),\n",
       " LabeledSentence(words=['one', 'face', 'ex', 'steal', 'show', 'yr', 'sxsw', 'appl', 'school', 'mkt', 'expert'], tags=['tweet_2']),\n",
       " LabeledSentence(words=['iphon', 'sxsw', 'app', 'would', 'b', 'pretti', 'awesom', 'crash', 'everi', 'min', 'extend', 'brows', 'fuckit', 'illmakeitwork'], tags=['tweet_3']),\n",
       " LabeledSentence(words=['line', 'outsid', 'appl', 'store', 'austin', 'wait', 'new', 'ipad', 'sxsw'], tags=['tweet_4']),\n",
       " LabeledSentence(words=['technew', 'one', 'lone', 'dude', 'await', 'ipad', 'appl', 'sxsw', 'store', 'tech', 'news', 'appl', 'ipad', 'sxsw', 'tablet', 'tech'], tags=['tweet_5'])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_tweets[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:574: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "model_d2v = gensim.models.Doc2Vec(dm=1, # dm = 1 for ‘distributed memory’ model \n",
    "dm_mean=1, # dm = 1 for using mean of the context word vectors\n",
    "size=500, # no. of desired features\n",
    "window=5, # width of the context window  \n",
    "negative=7, # if > 0 then negative sampling will be used \n",
    "min_count=5, # Ignores all words with total frequency lower than 2. \n",
    "workers=3, # no. of cores\n",
    "alpha=0.55, # learning rate \n",
    "seed = 25)                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 9093/9093 [00:00<00:00, 1140923.96it/s]\n"
     ]
    }
   ],
   "source": [
    "model_d2v.build_vocab([i for i in tqdm(labeled_tweets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d2v.train(labeled_tweets, total_examples= len(combi['tidy_tweet']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 500)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docvec_arrays = np.zeros((len(tokenized_tweet), 500)) \n",
    "for i in range(len(combi)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,500))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "docvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow = bow[:7274,:] \n",
    "test_bow = bow[7274:,:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['sentiment'],random_state=42,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6349416029569277"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "lreg = LogisticRegression() # training the model \n",
    "lreg.fit(xtrain_bow, ytrain) \n",
    "prediction = lreg.predict(xvalid_bow) # predicting on the validation set prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0  \n",
    "f1_score(yvalid, prediction,average='weighted') # calculating f1 score for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6222796825503487"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfidf = tfidf[:7274,:]\n",
    "test_tfidf = tfidf[7274:,:] \n",
    "xtrain_tfidf = train_tfidf[ytrain.index] \n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "lreg.fit(xtrain_tfidf, ytrain) \n",
    "prediction = lreg.predict(xvalid_tfidf)  \n",
    "f1_score(yvalid, prediction,average='weighted') # calculating f1 score for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5959027944614286"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_w2v = wordvec_df.iloc[:7274,:] \n",
    "test_w2v = wordvec_df.iloc[7274:,:] \n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:] \n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "lreg.fit(xtrain_w2v, ytrain) \n",
    "prediction = lreg.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6341926403862831"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_bow, ytrain) \n",
    "prediction = svc.predict(xvalid_bow) \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6222897566094161"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear',C=1, probability=True).fit(xtrain_tfidf, ytrain) \n",
    "prediction = svc.predict(xvalid_tfidf) \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6066060859676815"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, probability=True).fit(xtrain_w2v, ytrain) \n",
    "prediction = svc.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6520672570125686"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain) \n",
    "prediction = rf.predict(xvalid_bow) \n",
    "# validation score \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6034967719239407"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain) \n",
    "prediction = rf.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6503633261485824"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier(max_depth=10, n_estimators=1000,learning_rate=0.3).fit(xtrain_bow, ytrain) \n",
    "prediction = xgb_model.predict(xvalid_bow) \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6223585860778325"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain) \n",
    "prediction = xgb.predict(xvalid_w2v) \n",
    "f1_score(yvalid, prediction,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(xtrain_w2v, label=ytrain) \n",
    "dvalid = xgb.DMatrix(xvalid_w2v, label=yvalid) \n",
    "dtest = xgb.DMatrix(test_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective':'multi:softmax',\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.4,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    'num_class':4, \n",
    "     \n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eval(preds, dtrain):\n",
    "    labels = dtrain.get_label().astype(np.int)\n",
    "    preds = (preds >= 0.3).astype(np.int)\n",
    "    return [('f1_score', f1_score(labels, preds,average='weighted'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(6,20)\n",
    "     for min_child_weight in range(5,15)\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with max_depth=6, min_child_weight=5\n",
      "CV with max_depth=6, min_child_weight=6\n",
      "CV with max_depth=6, min_child_weight=7\n",
      "CV with max_depth=6, min_child_weight=8\n",
      "CV with max_depth=6, min_child_weight=9\n",
      "CV with max_depth=6, min_child_weight=10\n",
      "CV with max_depth=6, min_child_weight=11\n",
      "CV with max_depth=6, min_child_weight=12\n",
      "CV with max_depth=6, min_child_weight=13\n",
      "CV with max_depth=6, min_child_weight=14\n",
      "CV with max_depth=7, min_child_weight=5\n",
      "CV with max_depth=7, min_child_weight=6\n",
      "CV with max_depth=7, min_child_weight=7\n",
      "CV with max_depth=7, min_child_weight=8\n",
      "CV with max_depth=7, min_child_weight=9\n",
      "CV with max_depth=7, min_child_weight=10\n",
      "CV with max_depth=7, min_child_weight=11\n",
      "CV with max_depth=7, min_child_weight=12\n",
      "CV with max_depth=7, min_child_weight=13\n",
      "CV with max_depth=7, min_child_weight=14\n",
      "CV with max_depth=8, min_child_weight=5\n",
      "CV with max_depth=8, min_child_weight=6\n",
      "CV with max_depth=8, min_child_weight=7\n",
      "CV with max_depth=8, min_child_weight=8\n",
      "CV with max_depth=8, min_child_weight=9\n",
      "CV with max_depth=8, min_child_weight=10\n",
      "CV with max_depth=8, min_child_weight=11\n",
      "CV with max_depth=8, min_child_weight=12\n",
      "CV with max_depth=8, min_child_weight=13\n",
      "CV with max_depth=8, min_child_weight=14\n",
      "CV with max_depth=9, min_child_weight=5\n",
      "CV with max_depth=9, min_child_weight=6\n",
      "CV with max_depth=9, min_child_weight=7\n",
      "CV with max_depth=9, min_child_weight=8\n",
      "CV with max_depth=9, min_child_weight=9\n",
      "CV with max_depth=9, min_child_weight=10\n",
      "CV with max_depth=9, min_child_weight=11\n",
      "CV with max_depth=9, min_child_weight=12\n",
      "CV with max_depth=9, min_child_weight=13\n",
      "CV with max_depth=9, min_child_weight=14\n",
      "CV with max_depth=10, min_child_weight=5\n",
      "CV with max_depth=10, min_child_weight=6\n",
      "CV with max_depth=10, min_child_weight=7\n",
      "CV with max_depth=10, min_child_weight=8\n",
      "CV with max_depth=10, min_child_weight=9\n",
      "CV with max_depth=10, min_child_weight=10\n",
      "CV with max_depth=10, min_child_weight=11\n",
      "CV with max_depth=10, min_child_weight=12\n",
      "CV with max_depth=10, min_child_weight=13\n",
      "CV with max_depth=10, min_child_weight=14\n",
      "CV with max_depth=11, min_child_weight=5\n",
      "CV with max_depth=11, min_child_weight=6\n",
      "CV with max_depth=11, min_child_weight=7\n",
      "CV with max_depth=11, min_child_weight=8\n",
      "CV with max_depth=11, min_child_weight=9\n",
      "CV with max_depth=11, min_child_weight=10\n",
      "CV with max_depth=11, min_child_weight=11\n",
      "CV with max_depth=11, min_child_weight=12\n",
      "CV with max_depth=11, min_child_weight=13\n",
      "CV with max_depth=11, min_child_weight=14\n",
      "CV with max_depth=12, min_child_weight=5\n",
      "CV with max_depth=12, min_child_weight=6\n",
      "CV with max_depth=12, min_child_weight=7\n",
      "CV with max_depth=12, min_child_weight=8\n",
      "CV with max_depth=12, min_child_weight=9\n",
      "CV with max_depth=12, min_child_weight=10\n",
      "CV with max_depth=12, min_child_weight=11\n",
      "CV with max_depth=12, min_child_weight=12\n",
      "CV with max_depth=12, min_child_weight=13\n",
      "CV with max_depth=12, min_child_weight=14\n",
      "CV with max_depth=13, min_child_weight=5\n",
      "CV with max_depth=13, min_child_weight=6\n",
      "CV with max_depth=13, min_child_weight=7\n",
      "CV with max_depth=13, min_child_weight=8\n",
      "CV with max_depth=13, min_child_weight=9\n",
      "CV with max_depth=13, min_child_weight=10\n",
      "CV with max_depth=13, min_child_weight=11\n",
      "CV with max_depth=13, min_child_weight=12\n",
      "CV with max_depth=13, min_child_weight=13\n",
      "CV with max_depth=13, min_child_weight=14\n",
      "CV with max_depth=14, min_child_weight=5\n",
      "CV with max_depth=14, min_child_weight=6\n",
      "CV with max_depth=14, min_child_weight=7\n",
      "CV with max_depth=14, min_child_weight=8\n",
      "CV with max_depth=14, min_child_weight=9\n",
      "CV with max_depth=14, min_child_weight=10\n",
      "CV with max_depth=14, min_child_weight=11\n",
      "CV with max_depth=14, min_child_weight=12\n",
      "CV with max_depth=14, min_child_weight=13\n",
      "CV with max_depth=14, min_child_weight=14\n",
      "CV with max_depth=15, min_child_weight=5\n",
      "CV with max_depth=15, min_child_weight=6\n",
      "CV with max_depth=15, min_child_weight=7\n",
      "CV with max_depth=15, min_child_weight=8\n",
      "CV with max_depth=15, min_child_weight=9\n",
      "CV with max_depth=15, min_child_weight=10\n",
      "CV with max_depth=15, min_child_weight=11\n",
      "CV with max_depth=15, min_child_weight=12\n",
      "CV with max_depth=15, min_child_weight=13\n",
      "CV with max_depth=15, min_child_weight=14\n",
      "CV with max_depth=16, min_child_weight=5\n",
      "CV with max_depth=16, min_child_weight=6\n",
      "CV with max_depth=16, min_child_weight=7\n",
      "CV with max_depth=16, min_child_weight=8\n",
      "CV with max_depth=16, min_child_weight=9\n",
      "CV with max_depth=16, min_child_weight=10\n",
      "CV with max_depth=16, min_child_weight=11\n",
      "CV with max_depth=16, min_child_weight=12\n",
      "CV with max_depth=16, min_child_weight=13\n",
      "CV with max_depth=16, min_child_weight=14\n",
      "CV with max_depth=17, min_child_weight=5\n",
      "CV with max_depth=17, min_child_weight=6\n",
      "CV with max_depth=17, min_child_weight=7\n",
      "CV with max_depth=17, min_child_weight=8\n",
      "CV with max_depth=17, min_child_weight=9\n",
      "CV with max_depth=17, min_child_weight=10\n",
      "CV with max_depth=17, min_child_weight=11\n",
      "CV with max_depth=17, min_child_weight=12\n",
      "CV with max_depth=17, min_child_weight=13\n",
      "CV with max_depth=17, min_child_weight=14\n",
      "CV with max_depth=18, min_child_weight=5\n",
      "CV with max_depth=18, min_child_weight=6\n",
      "CV with max_depth=18, min_child_weight=7\n",
      "CV with max_depth=18, min_child_weight=8\n",
      "CV with max_depth=18, min_child_weight=9\n",
      "CV with max_depth=18, min_child_weight=10\n",
      "CV with max_depth=18, min_child_weight=11\n",
      "CV with max_depth=18, min_child_weight=12\n",
      "CV with max_depth=18, min_child_weight=13\n",
      "CV with max_depth=18, min_child_weight=14\n",
      "CV with max_depth=19, min_child_weight=5\n",
      "CV with max_depth=19, min_child_weight=6\n",
      "CV with max_depth=19, min_child_weight=7\n",
      "CV with max_depth=19, min_child_weight=8\n",
      "CV with max_depth=19, min_child_weight=9\n",
      "CV with max_depth=19, min_child_weight=10\n",
      "CV with max_depth=19, min_child_weight=11\n",
      "CV with max_depth=19, min_child_weight=12\n",
      "CV with max_depth=19, min_child_weight=13\n",
      "CV with max_depth=19, min_child_weight=14\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0. # initializing with 0 \n",
    "best_params = None \n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(max_depth,min_child_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = max_depth\n",
    "params['min_child_weight'] = min_child_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = xgb.cv(params,\n",
    "        dtrain,        feval= custom_eval,\n",
    "        num_boost_round=500,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=10,\n",
    "        \n",
    "    )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1 = cv_results['test-f1_score-mean'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tF1 Score 0.45463240000000005 for 27 rounds\n",
      "Best params: 19, 14, F1 Score: 0.45463240000000005\n"
     ]
    }
   ],
   "source": [
    "boost_rounds = cv_results['test-f1_score-mean'].argmax()    \n",
    "print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))    \n",
    "if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = (max_depth,min_child_weight) \n",
    "\n",
    "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = 9 \n",
    "params['min_child_weight'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with subsample=0.5, colsample=0.5\n",
      "\tF1 Score 0.45449660000000003 for 34 rounds\n",
      "CV with subsample=0.5, colsample=0.6\n",
      "\tF1 Score 0.45449660000000003 for 34 rounds\n",
      "CV with subsample=0.5, colsample=0.7\n",
      "\tF1 Score 0.45449660000000003 for 34 rounds\n",
      "CV with subsample=0.5, colsample=0.8\n",
      "\tF1 Score 0.45449660000000003 for 34 rounds\n",
      "CV with subsample=0.5, colsample=0.9\n",
      "\tF1 Score 0.45449660000000003 for 34 rounds\n",
      "CV with subsample=0.6, colsample=0.5\n",
      "\tF1 Score 0.4543118 for 24 rounds\n",
      "CV with subsample=0.6, colsample=0.6\n",
      "\tF1 Score 0.4543118 for 24 rounds\n",
      "CV with subsample=0.6, colsample=0.7\n",
      "\tF1 Score 0.4543118 for 24 rounds\n",
      "CV with subsample=0.6, colsample=0.8\n",
      "\tF1 Score 0.4543118 for 24 rounds\n",
      "CV with subsample=0.6, colsample=0.9\n",
      "\tF1 Score 0.4543118 for 24 rounds\n",
      "CV with subsample=0.7, colsample=0.5\n",
      "\tF1 Score 0.4548324 for 23 rounds\n",
      "CV with subsample=0.7, colsample=0.6\n",
      "\tF1 Score 0.4548324 for 23 rounds\n",
      "CV with subsample=0.7, colsample=0.7\n",
      "\tF1 Score 0.4548324 for 23 rounds\n",
      "CV with subsample=0.7, colsample=0.8\n",
      "\tF1 Score 0.4548324 for 23 rounds\n",
      "CV with subsample=0.7, colsample=0.9\n",
      "\tF1 Score 0.4548324 for 23 rounds\n",
      "CV with subsample=0.8, colsample=0.5\n",
      "\tF1 Score 0.4549367999999999 for 20 rounds\n",
      "CV with subsample=0.8, colsample=0.6\n",
      "\tF1 Score 0.4549367999999999 for 20 rounds\n",
      "CV with subsample=0.8, colsample=0.7\n",
      "\tF1 Score 0.4549367999999999 for 20 rounds\n",
      "CV with subsample=0.8, colsample=0.8\n",
      "\tF1 Score 0.4549367999999999 for 20 rounds\n",
      "CV with subsample=0.8, colsample=0.9\n",
      "\tF1 Score 0.4549367999999999 for 20 rounds\n",
      "CV with subsample=0.9, colsample=0.5\n",
      "\tF1 Score 0.452839 for 14 rounds\n",
      "CV with subsample=0.9, colsample=0.6\n",
      "\tF1 Score 0.452839 for 14 rounds\n",
      "CV with subsample=0.9, colsample=0.7\n",
      "\tF1 Score 0.452839 for 14 rounds\n",
      "CV with subsample=0.9, colsample=0.8\n",
      "\tF1 Score 0.452839 for 14 rounds\n",
      "CV with subsample=0.9, colsample=0.9\n",
      "\tF1 Score 0.452839 for 14 rounds\n",
      "Best params: 0.8, 0.5, F1 Score: 0.4549367999999999\n"
     ]
    }
   ],
   "source": [
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(5,10)]\n",
    "    for colsample in [i/10. for i in range(5,10)] ]\n",
    "max_f1 = 0. \n",
    "best_params = None \n",
    "for subsample, colsample in gridsearch_params:\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))\n",
    "     # Update our parameters\n",
    "    params['colsample'] = colsample\n",
    "    params['subsample'] = subsample\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        feval= custom_eval,\n",
    "        num_boost_round=200,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "     # Finding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = (subsample, colsample) \n",
    "\n",
    "print(\"Best params: {}, {}, F1 Score: {}\".format(best_params[0], best_params[1], max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = .9 \n",
    "params['colsample_bytree'] = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV with eta=0.3\n",
      "\tF1 Score 0.45409000000000005 for 75 rounds\n",
      "CV with eta=0.2\n",
      "\tF1 Score 0.45405359999999995 for 57 rounds\n",
      "CV with eta=0.1\n",
      "\tF1 Score 0.4532662 for 41 rounds\n",
      "CV with eta=0.05\n",
      "\tF1 Score 0.452609 for 82 rounds\n",
      "CV with eta=0.01\n",
      "\tF1 Score 0.4448506 for 0 rounds\n",
      "CV with eta=0.005\n",
      "\tF1 Score 0.44496499999999994 for 2 rounds\n",
      "Best params: 0.3, F1 Score: 0.45409000000000005\n"
     ]
    }
   ],
   "source": [
    "max_f1 = 0. \n",
    "best_params = None \n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))\n",
    "     # Update ETA\n",
    "    params['eta'] = eta\n",
    "\n",
    "     # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        feval= custom_eval,\n",
    "        num_boost_round=1000,\n",
    "        maximize=True,\n",
    "        seed=16,\n",
    "        nfold=5,\n",
    "        early_stopping_rounds=20\n",
    "    )\n",
    "\n",
    "     # Finding best F1 Score\n",
    "    mean_f1 = cv_results['test-f1_score-mean'].max()\n",
    "    boost_rounds = cv_results['test-f1_score-mean'].argmax()\n",
    "    print(\"\\tF1 Score {} for {} rounds\".format(mean_f1, boost_rounds))\n",
    "    if mean_f1 > max_f1:\n",
    "        max_f1 = mean_f1\n",
    "        best_params = eta \n",
    "print(\"Best params: {}, F1 Score: {}\".format(best_params, max_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample': 0.9,\n",
       " 'colsample_bytree': 0.5,\n",
       " 'eta': 0.2,\n",
       " 'max_depth': 9,\n",
       " 'min_child_weight': 7,\n",
       " 'objective': 'multi:softmax',\n",
       " 'subsample': 0.9}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'colsample': 0.9,\n",
    " 'colsample_bytree': 0.5, 'eta': 0.2,\n",
    " 'max_depth': 9, 'min_child_weight': 7,\n",
    " 'objective': 'multi:softmax',\n",
    " 'subsample': 0.9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:09:24] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { colsample } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\tValidation-merror:0.41090\tValidation-f1_score:0.45158\n",
      "Multiple eval metrics have been passed: 'Validation-f1_score' will be used for early stopping.\n",
      "\n",
      "Will train until Validation-f1_score hasn't improved in 10 rounds.\n",
      "[1]\tValidation-merror:0.38342\tValidation-f1_score:0.45645\n",
      "[2]\tValidation-merror:0.36967\tValidation-f1_score:0.45883\n",
      "[3]\tValidation-merror:0.37196\tValidation-f1_score:0.45551\n",
      "[4]\tValidation-merror:0.37242\tValidation-f1_score:0.45628\n",
      "[5]\tValidation-merror:0.37059\tValidation-f1_score:0.45704\n",
      "[6]\tValidation-merror:0.36601\tValidation-f1_score:0.45671\n",
      "[7]\tValidation-merror:0.36097\tValidation-f1_score:0.45784\n",
      "[8]\tValidation-merror:0.35593\tValidation-f1_score:0.45784\n",
      "[9]\tValidation-merror:0.35822\tValidation-f1_score:0.45839\n",
      "[10]\tValidation-merror:0.36097\tValidation-f1_score:0.45784\n",
      "[11]\tValidation-merror:0.35639\tValidation-f1_score:0.45635\n",
      "[12]\tValidation-merror:0.35960\tValidation-f1_score:0.45635\n",
      "Stopping. Best iteration:\n",
      "[2]\tValidation-merror:0.36967\tValidation-f1_score:0.45883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    feval= custom_eval,\n",
    "    num_boost_round= 1000,\n",
    "    maximize=True,\n",
    "    evals=[(dvalid, \"Validation\")],\n",
    "    early_stopping_rounds=10\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = xgb_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 2., ..., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
